---
title: "Regression"
author: "Alan T. Arnholt"
date: "April 10, 2015"
output: html_document
---

```{r, label = "Setup", echo = FALSE, message = FALSE}
knitr::opts_chunk$set(comment = NA, message = FALSE, warning = FALSE, fig.align = "center")
```


Consider the data frame `signdist` in the `PDS` package used in chapter 13.

```{r}
library(PDS)
head(signdist)
plot(Distance ~ Age, data = signdist)
mod <- lm(Distance ~ Age, data = signdist)
summary(mod)
abline(mod)
cor(signdist$Distance, signdist$Age)
cor.test(signdist$Distance, signdist$Age)
```

Doing similar things with `ggplot2`.

```{r}
library(ggplot2)
ggplot(data = signdist, aes(x = Age, y = Distance)) + 
  geom_point() +
  theme_bw() +
  geom_smooth(method = "lm") + 
  annotate("text", x = 60, y = 550, label="widehat(Distance)==576.6819 -3.0068*Age", parse = TRUE) +
  geom_segment(aes(x = 60, y = 280, xend = 60, yend = 576.6819 -3.0068*60), linetype = "dashed") + 
  geom_segment(x = 60, y = 576.6819 -3.0068*60, xend = 10, yend = 576.6819 -3.0068*60, linetype = "dashed")
```

```{r}
predict(mod, newdata = data.frame(Age = 60), interval = "confidence", level = 0.90)
plot(mod, which = 1)
plot(mod, which = 2)
NDF <- fortify(mod)
head(NDF)
ggplot(data = NDF, aes(x = .fitted, y = .resid)) +
  geom_point(color = "blue", size = 3) + 
  theme_bw() + 
  geom_hline(y = 0, linetype = "dashed") + 
  labs(x = "Fitted values", y = "Residuals")
```

## Dichotomous Response Variable (Logistic Regression)
(Example taken from *An Introduction to Statistical Learning*)

> Goal:  Predict whether an individual will default on his or her credit card payment.

```{r}
library(ISLR)
head(Default)
ggplot(data = Default, aes(x = balance, y = income, colour = default)) + 
  geom_point(alpha = 0.8) + 
  theme_bw() + 
  scale_color_brewer(palette=7)
```

```{r}
ggplot(data = Default, aes(x = default, y = balance, fill = default)) + 
  geom_boxplot() + 
  theme_bw() + 
  guides(fill = FALSE) + 
  scale_fill_brewer(palette=7)
ggplot(data = Default, aes(x = default, y = income, fill = default)) + 
  geom_boxplot() + 
  theme_bw() + 
  guides(fill = FALSE) + 
  scale_fill_brewer(palette=7)
```

> Why not regression?

```{r}
Default$defaultN <- ifelse(Default$default == "No", 0, 1)
Default$studentN <- ifelse(Default$student =="No", 0, 1)
summary(Default$defaultN)
head(Default)
ggplot(data = Default, aes(x = balance, y = defaultN)) + 
  geom_point(alpha = 0.5) + 
  theme_bw() + 
  stat_smooth(method = "lm") +
  labs(y = "Probability of Default")
```

Some estimated probabilities are negative! For balances close to zero, we predict a negative probability of default.  For very large balances, we get values greater than 1. 

## Logistic regression

What we need is a function of $X$ that returns values between 0 and 1.  Consider the *logistic function*

$$p(X) = \frac{e^{\beta_0 + \beta_1X}}{1 + e^{\beta_0 + \beta_1X}}$$

which will always produce an *S-shaped* curve regardless of the value of $X$.


Let $y = \beta_0 + \beta_1X$, then

$p(X)=\frac{e^y}{1 + e^y} \rightarrow p(X) + e^{y}p(X) = e^{y} \rightarrow
p(X) = e^{y} - e^{y}p(X) \rightarrow \frac{p(X)}{1 - p(X)}=e^{y} = e^{\beta_0 + \beta_1X}$

$$\text{log}\left(\frac{p(X)}{1 - p(X)}\right) = \beta_0 + \beta_1X$$

The quantity $\text{log}\left(\frac{p(X)}{1 - p(X)}\right)$ is called the *log-odds* or *logit*, and the quantity $\frac{p(X)}{1 - p(X)}$ is called the odds.  

NOTE:  In regression, $\beta_1$ gives the average change in $Y$ associated with a one-unit increase in $X$.  In a logistic regression model, increasing $X$ by one unit changes the log odds by $\beta_1$, or equivalently it multiplies the odds by $e^{\beta_1}$.

```{r}
ggplot(data = Default, aes(x = balance, y = defaultN)) + 
  geom_point(alpha = 0.5) + 
  theme_bw() + 
  stat_smooth(method = "glm", family = "binomial") +
  labs(y = "Probability of Default")
```

The probability of default given `balance` can be written as P(default = Yes | balance).

```{r}
log.mod <- glm(default ~ balance, data = Default, family = "binomial")
summary(log.mod)
```

From the output note that $\hat{\beta} = `r summary(log.mod)$coef[2, 1]`$.  This value indicates that an increase in `balance` is associated with an increase in the log odds of `default` by `r summary(log.mod)$coef[2, 1]` units.

## Making predictions

Using the estimated coefficients, the predicted probability of default for an individual with a balance of $1,500 is

$$\hat{p}(X) =  \frac{e^{\hat{\beta_0} + \hat{\beta}_1X}}{1 + e^{\hat{\beta_0} + \hat{\beta_1}X}} = \frac{e^{`r summary(log.mod)$coef[1, 1]`+ `r summary(log.mod)$coef[2, 1]` \times 1500}}{1 + e^{`r summary(log.mod)$coef[1, 1]`+ `r summary(log.mod)$coef[2, 1]` \times 1500}} = `r exp(predict(log.mod, newdata = data.frame(balance = 1500)))/(1 + exp(predict(log.mod, newdata = data.frame(balance = 1500))))`.$$

```{r}
predict(log.mod, newdata = data.frame(balance = 1500), type = "response")
```

For an individual with a `balance` of $2,500, the probability of default is

$$p(X) = `r exp(predict(log.mod, newdata = data.frame(balance = 2500)))/(1 + exp(predict(log.mod, newdata = data.frame(balance = 2500))))`$$.

```{r}
predict(log.mod, newdata = data.frame(balance = 2500), type = "response")
```

Are students more likely to default than non-students?

```{r}
mod2 <- glm(default ~ student, data = Default, family = "binomial")
summary(mod2)
```

Note that the coefficient associated with `studentYes` is positive, and the associated *p-value* is statistically significant.  This indicates that students tend to have higher default probabilities than non-students:

$$\widehat{Pr}(\text{default = Yes }|\text{ student = Yes}) = \frac{e^{`r summary(mod2)$coef[1, 1]`+ `r summary(mod2)$coef[2, 1]` \times 1}}{1 + e^{`r summary(mod2)$coef[1, 1]`+ `r summary(mod2)$coef[2, 1]` \times 1}} = `r exp(predict(mod2, newdata = data.frame(student = "Yes")))/(1 + exp(predict(mod2, newdata = data.frame(student = "Yes"))))`,$$

```{r}
predict(mod2, newdata = data.frame(student = "Yes"), type = "response")
```

$$\widehat{Pr}(\text{default = Yes }|\text{ student = No}) = \frac{e^{`r summary(mod2)$coef[1, 1]`+ `r summary(mod2)$coef[2, 1]` \times 0}}{1 + e^{`r summary(mod2)$coef[1, 1]`+ `r summary(mod2)$coef[2, 1]` \times 1}} = `r exp(predict(mod2, newdata = data.frame(student = "No")))/(1 + exp(predict(mod2, newdata = data.frame(student = "No"))))`.$$

```{r}
predict(mod2, newdata = data.frame(student = "No"), type = "response")
```


## Multiple Logistic Regression

Consider a model that uses `balance`, `income`, and `student` to predict `default`.

```{r}
mlog.mod <- glm(default ~ balance + income + student, data = Default, family = "binomial")
summary(mlog.mod)
```

There is a surprising result here.  The *p-value* associated with `balance` and the `student` are very small, indicating that each of these variables is associated with the probability of `default`.  However, the coefficient for `student` is negative, indicating that students are less likely to default than non-students.  Yet, the coefficient in `mod2` for `student` was positive.  How is it possible for the variable `student` to be associated with an *increase* in probability in `mod2` and a *decrease* in probability in `mlog.mod`?


```{r}
Default$PrDef <- exp(predict(mlog.mod))/(1 + exp(predict(mlog.mod)))
# Or the following is the same
Default$PRdef <- predict(mlog.mod, type = "response")
ggplot(data = Default, aes(x = balance, y = default, color = student)) + 
  geom_line(aes(x = balance, y = PrDef)) + 
  scale_color_brewer(palette = 7) + 
  theme_bw() +
  labs(y = "Probability of Default")
library(dplyr)
Default <- Default %>% 
  arrange(balance) %>% 
  mutate(balanceFAC = cut(balance, breaks = seq(500, 2655, length.out = 10), include.lower = TRUE))
head(Default)
T1 <- xtabs(~balanceFAC + default, data = Default, subset = student == "Yes")
T2 <- xtabs(~balanceFAC + default, data = Default, subset = student == "No")
SDR  <- T1[, 2]/apply(T1[, 1:2], 1, sum)
NSDR <- T2[, 2]/apply(T2[, 1:2], 1, sum)
BalanceM <- c(500+120, 739+120, 979+120, 1220+120, 1460+120, 1700+120, 1940+120, 2180+120, 2420+120)
DF <- data.frame(SDR, NSDR, BalanceM)
DF
ggplot(data = DF, aes(x = BalanceM, y = NSDR)) +
  geom_line(color = "red") + 
  geom_line(aes(x = BalanceM, y = SDR), color = "green") + 
  theme_bw() +
  labs(y = "Default Rate", x = "Credit Card Balance") 
```

## Odds Ratio (OR)

The *odds ratio*, denoted OR, is the ratio of the odds for $X = 1$ to the odds for $X = 0$, and is given by the equation

$$\text{OR}=\frac{\frac{p(1)}{1 - p(1)}}{\frac{p(0)}{1 - p(0)}}.$$

Substituting  
$$\frac{e^{\beta_0 + \beta_1}}{1 + e^{\beta_0 + \beta_1}}$$ for $p(X=1)$, and
$$\frac{e^{\beta_0}}{1 + e^{\beta_0}}$$ for $p(X=0)$, respectively, one can show that
$$\text{OR} = e^{\beta_1}.$$

The odds ratio is widely used as a measure of association as it approximates how much more likely or unlikely (in terms of odds) it is for the outcome to be present among those subjects with $X = 1$ as compared to those subjects with $X = 0$.  

Using the `mlog.mod` model, the response variable $Y$  is `default`, which denotes whether a credit card holder has defaulted (Yes/1) or not defaulted (No/0) on his/her current payment. The independent variable $X$  is `student`, which denotes whether the individual is a student (Yes/1) or is not a student (No/1).  A hypothetical odds ratio in this scenario of 2 is interpreted to mean that the odds of default among students is two times greater than the odds of default among non-students.  

An estimate of the odds ratio is

$$\widehat{\text{OR}} = e^{-0.6467758} = `r exp(coef(summary(mlog.mod))[4,1])`.$$

This is interpreted to mean that the odds of default among students is `r exp(coef(summary(mlog.mod))[4,1])` the odds of default among non-students.
