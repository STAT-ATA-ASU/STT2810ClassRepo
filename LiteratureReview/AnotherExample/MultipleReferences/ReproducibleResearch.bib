
@article{peng_reproducible_2006,
	title = {Reproducible {Epidemiologic} {Research}},
	volume = {163},
	issn = {0002-9262, 1476-6256},
	url = {http://0-aje.oxfordjournals.org.wncln.wncln.org/content/163/9/783},
	doi = {10.1093/aje/kwj093},
	abstract = {The replication of important findings by multiple independent investigators is fundamental to the accumulation of scientific evidence. Researchers in the biologic and physical sciences expect results to be replicated by independent data, analytical methods, laboratories, and instruments. Epidemiologic studies are commonly used to quantify small health effects of important, but subtle, risk factors, and replication is of critical importance where results can inform substantial policy decisions. However, because of the time, expense, and opportunism of many current epidemiologic studies, it is often impossible to fully replicate their findings. An attainable minimum standard is “reproducibility,” which calls for data sets and software to be made available for verifying published findings and conducting alternative analyses. The authors outline a standard for reproducibility and evaluate the reproducibility of current epidemiologic research. They also propose methods for reproducible research and implement them by use of a case study in air pollution and health.},
	language = {en},
	number = {9},
	urldate = {2015-08-21},
	journal = {American Journal of Epidemiology},
	author = {Peng, Roger D. and Dominici, Francesca and Zeger, Scott L.},
	month = may,
	year = {2006},
	pmid = {16510544},
	keywords = {air pollution, information dissemination, models, statistical, NMMAPS, National Morbidity, Mortality, and Air Pollution Study},
	pages = {783--789},
	file = {Full Text PDF:/Users/alan/Library/Application Support/Firefox/Profiles/rvi3l9fn.default/zotero/storage/25I4ZT2W/Peng et al. - 2006 - Reproducible Epidemiologic Research.pdf:application/pdf;Snapshot:/Users/alan/Library/Application Support/Firefox/Profiles/rvi3l9fn.default/zotero/storage/W9VES3UT/783.html:text/html}
}

@article{fomel_reproducible_2009,
	title = {Reproducible {Research}},
	volume = {11},
	issn = {1521-9615},
	url = {http://scitation.aip.org/content/aip/journal/cise/11/1/10.1109/MCSE.2009.14},
	doi = {10.1109/MCSE.2009.14},
	abstract = {Reproducibility is a core principle of science. For computational experiments to become reproducible, one needs to develop a system for linking scientific publications with computational recipes. Articles in this special issue argue in favor of computational reproducibility and describe several practical approaches to reproducible research.},
	number = {1},
	urldate = {2015-08-21},
	journal = {Computing in Science \& Engineering},
	author = {Fomel, Sergey and Claerbout, Jon F.},
	month = jan,
	year = {2009},
	pages = {5--7},
	file = {Snapshot:/Users/alan/Library/Application Support/Firefox/Profiles/rvi3l9fn.default/zotero/storage/UXS2QFSG/MCSE.2009.html:text/html}
}

@article{leveqije_reproducible_2012,
	title = {Reproducible research for scientific computing: {Tools} and strategies for changing the culture},
	volume = {14},
	issn = {1521-9615},
	shorttitle = {Reproducible research for scientific computing},
	doi = {10.1109/MCSE.2012.38},
	abstract = {This article considers the obstacles involved in creating reproducible computational research as well as some efforts and approaches to overcome them.},
	number = {4},
	journal = {Computing in Science Engineering},
	author = {LeVeqije, R.J. and Mitchell, I.M. and Stodden, V.},
	month = jul,
	year = {2012},
	keywords = {Computational complexity, computational science and engineering, cultural change, data and code disclosure, Hidden Markov models, natural sciences computing, reproducibility, Reproducibility of Results, reproducible computational research, reproducible research, research and development, scientific computing},
	pages = {13--17},
	file = {IEEE Xplore Abstract Record:/Users/alan/Library/Application Support/Firefox/Profiles/rvi3l9fn.default/zotero/storage/9K5VIU8N/articleDetails.html:text/html}
}

@article{peng_reproducible_2011,
	title = {Reproducible {Research} in {Computational} {Science}},
	volume = {334},
	issn = {0036-8075},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3383002/},
	doi = {10.1126/science.1213847},
	abstract = {Computational science has led to exciting new developments, but the nature of the work has exposed limitations in our ability to evaluate published findings. Reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible.},
	number = {6060},
	urldate = {2015-08-21},
	journal = {Science (New York, N.y.)},
	author = {Peng, Roger D.},
	month = dec,
	year = {2011},
	pmid = {22144613},
	pmcid = {PMC3383002},
	pages = {1226--1227},
	file = {PubMed Central Full Text PDF:/Users/alan/Library/Application Support/Firefox/Profiles/rvi3l9fn.default/zotero/storage/HKHB5HR8/Peng - 2011 - Reproducible Research in Computational Science.pdf:application/pdf}
}

@article{hutton_towards_2015,
	title = {Towards reproducibility in online social network research},
	volume = {PP},
	issn = {2168-6750},
	doi = {10.1109/TETC.2015.2458574},
	abstract = {The challenge of conducting reproducible computational research is acknowledged across myriad disciplines from biology to computer science. In the latter, research leveraging online social networks (OSNs) must deal with a set of complex issues, such as ensuring data can be collected in an appropriate and reproducible manner. Making research reproducible is difficult, and researchers may need suitable incentives, and tools and systems, to do so. In this paper we explore the state-of-the-art in OSN research reproducibility, and present an architecture to aid reproducibility. We characterise reproducible OSN research using three main themes: reporting of methods, availability of code, and sharing of research data. We survey 505 papers and assess the extent to which they achieve these reproducibility objectives. While systems-oriented papers are more likely to explain data-handling aspects of their methodology, social science papers are better at describing their participant-handling procedures. We then examine incentives to make research reproducible, by conducting a citation analysis of these papers. We find that sharing data is associated with increased citation count, while sharing method and code does not appear to be. Finally, we introduce our architecture which supports the conduct of reproducible OSN research, which we evaluate by replicating an existing research study.},
	number = {99},
	journal = {IEEE Transactions on Emerging Topics in Computing},
	author = {Hutton, L. and Henderson, T.},
	year = {2015},
	keywords = {Data collection, Data sharing, Ethics, Facebook, Licenses, online social networks, reproducibility, Sociology, Twitter},
	pages = {1--1},
	file = {IEEE Xplore Abstract Record:/Users/alan/Library/Application Support/Firefox/Profiles/rvi3l9fn.default/zotero/storage/AGQBVDJS/abs_all.html:text/html;IEEE Xplore Full Text PDF:/Users/alan/Library/Application Support/Firefox/Profiles/rvi3l9fn.default/zotero/storage/EP7Q8XGT/Hutton and Henderson - 2015 - Towards reproducibility in online social network r.pdf:application/pdf}
}