\documentclass[final]{beamer}
\usefonttheme{serif}
\mode<presentation>{\usetheme{ASU}}
\usepackage{amsmath, amsfonts, amssymb, pxfonts, eulervm, xspace, enumerate, hyperref, color, bookmark}
\usepackage{graphicx}
\usepackage[orientation=landscape, size=a0, scale=1.4, debug]{beamerposter}
% \usepackage{natbib}

\usecolortheme{rose}
\setbeamercolor{background canvas}{bg=magenta!16!yellow!90}

\beamertemplategridbackground[1cm]

%-- Header and footer information ----------------------------------
\newcommand{\footright}{\href{https://github.com/alanarnholt/STT3851-SP2015-Arnholt-Alan}{https://github.com/alanarnholt/STT3851-SP2015-Arnholt-Alan}}
\newcommand{\footleft}{\href{mailto:arnholtat@appstate.edu}{Faculty Advisor: Alan Arnholt}}

\def\conference{16\textsuperscript{th} Annual Celebration of Student Research and Creative Endeavors}
\title{Using Statistical Learning to Predict North Carolina County Voting Patterns}
\author{Will Callaway, Eitan Lees, Maureen O'Donnell} 
\institute{Department of Mathematical Sciences}
%-------------------------------------------------------------------


%-- Main Document --------------------------------------------------
\begin{document}
\begin{frame}[fragile]
\vspace{-2ex}
\begin{columns}[t]

<<setup, comment = NA, echo = FALSE, message = FALSE, warning=FALSE>>=
options(width = 35)
opts_chunk$set(comment = NA, fig.align='center', fig.width=13, fig.height=6, cache=FALSE, warning = FALSE, message = FALSE)
require(maps)
require(ggplot2)
require(PASWR)
require(tree)
require(randomForest)
require(boot)
require(xtable)
@

%-- Column 1 ---------------------------------------------------
\begin{column}{0.31\linewidth}
\begin{minipage}[t][.955\textheight]{\linewidth} 
%-- Block 1-1
% \vspace{0ex}
\begin{block}{Overview}
\begin{itemize}
\item On May 8, 2012, North Carolina voters approved Amendment One.  This poster examines four different models used to predict North Carolina county voting behavior.  
\item To ensure accurate predictive power for future observations, the data are split into a training set (80\%) and a test set (20\%). 
\item Root mean squared error of the test set is used as a measure of model adequacy.  
\item All computations and graphs are created with the open source software \texttt{R} \cite{R-base}. 
\end{itemize}
\vspace{0ex}
\end{block}
\vfill

%-- Block 1-2
\begin{block}{K-Fold Cross-Validation}
\begin{itemize}
\item Cross validation is the simplest and most widely used method for estimating prediction error \cite{JF09}.  This method directly estimates the expected extra-sample error, $Err = E[{L(Y, \hat{f\,}\!(X))}]$.  In this work, the loss function, $L$, is the square root of the average squared error loss.
\vspace{2ex}
\item The data in this project is split into $K=10$ equal sized parts.  The cross-validation estimate of the prediction error is $$CV(\hat{f\,}\!)=\frac{1}{N}\sum_{i=1}^{N}L(y_i, \hat{f\,}\!^{-K(i)}(x_i)),$$
where $\hat{f\,}\!^{-K}(x)$ denotes the fitted function with the $K$\textsuperscript{th} part of the data removed.
\end{itemize}
\vspace{0ex}
\vfill
\end{block}
\vfill

%-- Block 1-3
\begin{block}{Basic Models Used}
\begin{enumerate}[I.]
\item Least Squares Regression
\vspace{2ex}

Note: Variables are described in the Variable Table handout.
\begin{enumerate}[a.]
\item Model from \cite{DE12} applied to Training data (\textcolor{blue}{mod1A}) --- Percent voting for Amendment One is modeled using the predictors pct18.24, medinc, pctb, mccain08, evanrate, pctrural, and pctba. 
\item Our OLS model applied to Training data (\textcolor{blue}{mod1B}) --- Percent voting for Amendment One is modeled using the predictors obama08, pctrural, pctw, pctd, pctb, log(pct18.24), log(pctcolenrol), pctfm, log(pctfd), pctown,  medinc, $\text{medinc}^2$, evanrate, $\text{evanrate}^2$, pctfb, $\text{pctfb}^2$, log(pctstud), and log(colden).
\end{enumerate}
\item Cross validated, $K = 10$, and pruned, $n_{\text{leaves}}=4$, regression tree \cite{JF09} (\textcolor{blue}{mod2})
\item Random Forest built from, $n_{\text{trees}} = 5000 $, \cite{AL02} (\textcolor{blue}{mod3})
\end{enumerate}
\vspace{0ex}
<<REGmodels, echo = FALSE>>=
NCV <- read.csv("../Data/NCV03-29-13.csv")
set.seed(13)
train_index <- sample(1:100,size=80)
train1_NCV <- NCV[train_index,]
test1_NCV <- NCV[-train_index,]
train_NCV <- train1_NCV[,-c(1,6,7,11,13,14,15,16,17,18,
                           19,20,33,34,35,36,37,38,39,40,49,50)]
test_NCV <- test1_NCV[,-c(1,6,7,11,13,14,15,16,17,18,
                           19,20,33,34,35,36,37,38,39,40,49,50)]
BJmod <- glm(pctfor ~ pct18.24 + medinc + pctb + mccain08 + evanrate + pctrural + pctba, data = train_NCV, family = gaussian)
# summary(BJmod)
# removed some variables...some more to consider???:  medinc + I(medinc^2) + evanrate + I(evanrate^2) + pctfb + I(pctfb^2) + log(pctstud + 1e-6)  + log(colden + 1e-6)
EMmod <- glm(pctfor ~ obama08 + pctrural + pctw + pctd + pctb + log(pct18.24 + 1e-6)  + log(pctcolenrol + 1e-6) + pctfm + log(pctfd + 1e-6) + pctown + medinc + I(medinc^2) + evanrate + I(evanrate^2) + pctfb + I(pctfb^2) + log(pctstud + 1e-6)  + log(colden + 1e-6) , data=train_NCV, family = gaussian)

RMSEmod1aNoCV <- sqrt(mean(resid(BJmod)^2))
# cross-validated
val.10.fold <- cv.glm(data = train_NCV, glmfit = BJmod, K = 10)
RMSEmod1a <- val.10.fold$delta[1]^.5
val.10.foldEM <- cv.glm(data = train_NCV, glmfit = EMmod, K = 10)
RMSEmod1b <- val.10.foldEM$delta[1]^.5
@
\end{block}
\vfill

\end{minipage}
\end{column}%1

%-- Column 2 ---------------------------------------------------

\begin{column}{0.31\linewidth}
\begin{minipage}[t][.955\textheight]{\linewidth} 
%-- Block 2-1
%\vspace{0ex}
\begin{block}{Cross Validated and Pruned Tree}
%\vspace{0ex}
<<TREE, echo = FALSE, fig.width = 13, fig.height= 5>>=	  
options(digits = 7)
set.seed(21)
ncTree <- tree(pctfor ~ . , data = train_NCV)
# plot(cv.tree(ncTree, FUN = prune.tree, method = "deviance"))
pruneTree <- prune.tree(ncTree, best = 4)
plot(pruneTree)
text(pruneTree, cex = 1.5)
RMSEmod2 <- sqrt(mean(resid(pruneTree)^2))
rfMod <- randomForest(pctfor ~., data = train_NCV, ntree = 5000, importance = TRUE, na.action = na.omit, proximity = TRUE)
# print(rfMod)
NCV$RFpctfor <- predict(rfMod, newdata = NCV)
NCV$EMWpctfor <- predict(EMmod, newdata = NCV)
@
\vspace{-3ex}
\end{block}
\vfill

%-- Block 2-2
%\vspace{3ex}
\begin{block}{North Carolina Maps}
\vspace{-2ex}
<<NCmap, echo = FALSE, fig.width = 13, fig.height=6.5>>=
# NCV <- read.csv("../../NCV03-29-13.csv")
NC_map <- map_data('county', 'north carolina') 
# Need to force COUNTY to lowercase
MERGED <- merge(NC_map, NCV, by.x = "subregion", by.y = "countyName")
# dim(MERGED)
p2 <- ggplot(data = MERGED, aes(x = long, y = lat, group = group, fill = pctfor)) # set up data
p3 <- ggplot(data = MERGED, aes(x = long, y = lat, group = group, fill = pctba*100))
##
p4 <- ggplot(data = MERGED, aes(x = long, y = lat, group = group, fill = EMWpctfor))
##

theme_clean <- function(base_size = 12){
  require(grid)  # need for unit() function
    theme_grey(base_size) %+replace%
    theme(
      axis.title         = element_blank(),
      axis.text          = element_blank(),
      panel.background   = element_blank(),
      panel.grid         = element_blank(),
      axis.ticks.length  = unit(0, "cm"),
      axis.ticks.margin  = unit(0.01, "cm"),
      panel.margin       = unit(c(0,0,0,0), "lines"),
      plot.margin        = unit(c(0, 0, 0, 0), "lines"),
      complete = TRUE
    )
}
p2 + geom_polygon(color="white") + scale_fill_gradient2(low = "blue", mid ="grey90", high = "red", midpoint= 50) + coord_map("polyconic") + theme_clean() + labs(fill = "Percent Voting For\nAmendment One")  #+ ggtitle("North Carolina Counties")
p3 + geom_polygon(color="white") + scale_fill_gradient2(low = "red", mid="grey90", high = "blue", midpoint=16) + coord_map("polyconic") + theme_clean() + labs(fill = "Percent with\nBA/BS degree\nor Higher Degree")  #+ ggtitle("North Carolina Counties")
p4 + geom_polygon(color="white") + scale_fill_gradient2(low="blue", mid="grey90", high="red", midpoint= 50) + coord_map("polyconic") + theme_clean() + labs(fill = "Prediction of\nPercent Voting For\nAmendment One\nBased on mod1b") # +  ggtitle("North Carolina Counties")
@
\vspace{-2ex}
\end{block}
\vfill

\end{minipage}
\end{column}%2

%-- Column 3 ---------------------------------------------------
\begin{column}{0.31\linewidth}
\begin{minipage}[t][.955\textheight]{\linewidth} 
%-- Block 3-1
\vspace{0ex}
\begin{block}{Random Forest Variable Importance}
\vspace{-6ex}
<<Rforest, echo = FALSE, fig.height=6.5>>=
varImpPlot(rfMod, main = "", n.var = 10, pch = 19, cex=1.4, col = "black")
RMSEmod3 <- sqrt(mean(rfMod$mse))
###########
RMSEtestmod1a <- sqrt(mean((predict(BJmod, newdata= test_NCV) - test_NCV$pctfor)^2))
RMSEtestmod1b <- sqrt(mean((predict(EMmod, newdata= test_NCV) - test_NCV$pctfor)^2))
RMSEtestmod2 <- sqrt(mean((predict(pruneTree, newdata= test_NCV) - test_NCV$pctfor)^2))
RMSEtestmod3 <- sqrt(mean((predict(rfMod, newdata = test_NCV) - test_NCV$pctfor)^2))
#############################
# predict(rfMod, newdata = NCV)
@
\vspace{-2ex}
\vfill
\end{block}
\vfill

%-- Block 3-2
\begin{block}{Prediction Errors}
<<Results, echo = FALSE, results='asis'>>=
x1 <- c(RMSEmod1a,  RMSEmod1b, RMSEmod2, RMSEmod3)
x2 <- c(RMSEtestmod1a, RMSEtestmod1b, RMSEtestmod2, RMSEtestmod3)
XR <- rbind(x1, x2)
dimnames(XR) <- list(c("Training Error", "Testing Error"), c('mod1A', 'mod1B', 'mod2', 'mod3'))
xtable(XR, align = rep("c", 5), caption ="Training and Testing Error are the RMSE computed from a $K=10$ cross-validated model for all models except mod3.  The RMSE for the random forest model (mod3) does not use cross-validation.", label ="PT")
@
\vspace{0ex}
\vfill
\end{block}
\vfill

%-- Block 3-3
\begin{block}{Further Directions}
\begin{itemize}
\item Use the models developed in this poster to predict county votes for states that have had similar marriage amendments such as South Carolina, Wisconsin, South Dakota, Florida, Idaho, Alabama, Utah, Michigan, Texas, Arkansas, Louisiana, Kansas, Kentucky, Ohio, and Nebraska. 
\item  Use ensemble methods (combining multiple models) for better prediction.
\item  Make our local maps available via the internet using the shiny server.
\end{itemize}
\vspace{0ex}
\vfill
\end{block}
\vfill

%-- Block 3-4
\begin{block}{References}
\footnotesize
\setbeamertemplate{bibliography item}[text]
\vspace{-1ex}
<<auto-bib, version=packageVersion('knitr'), echo=FALSE, cache=TRUE, message=FALSE, warning=FALSE>>=
# write all packages in the current session to a bib file
write_bib(c(.packages()), file = 'knitr-packages.bib')
@

\bibliographystyle{plain}  % can use plain but comment out natbib at top if using plain
\bibliography{knitr-packages,poster}
\normalsize
\vfill
\end{block} 
\vfill

\end{minipage}
\end{column}%3




\end{columns}
\end{frame}
\end{document}

