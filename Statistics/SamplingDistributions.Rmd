---
title: "Sampling Distributions"
author: "Alan T. Arnholt"
date: "March 16, 2015"
output: html_document
---

```{r, echo = FALSE, message = FALSE}
knitr::opts_chunk$set(comment = NA, message = FALSE, warning = FALSE, fig.align = "center")
```

A **sampling distribution** of a *statistic* is its probability distribution.  In other words, the sampling distribution of a statistic summarizes a data set, and represents how the statistic varies across random data sets.

The standard deviation of a statistic *T* is written $\sigma_{T} = SE[T]$.  An estimate for the standard deviation of a statistic *T* is written $\hat{\sigma}_{T} = \widehat{SE[T]}$.

**EXAMPLE:** Toss a fair coin $n = 10$ times and note the proportion of heads $\hat{p}$.  If you repeat the experiment, you probably would not get the same proportion of heads.  If you toss 50 sets of 10 coin flips, you might see outcomes (i.e., proportions of heads, $\hat{p}$) such as those below.

```{r sampDIST}
set.seed(12)
cointoss <- rbinom(50, 10, 1/2)
phat <- cointoss/10
xtabs(~phat)
plot(xtabs(~phat),ylab= "count",xlab = expression(hat(p)))
```
Although proportions between 0.3 and 0.7 occur most often, we see there is a proportion as low as 0.1 heads and as high as 0.8 heads.  Instead of 50 sets of 10 coin flips, the next simulation performs 50,000 sets of 10 tosses.

```{r sampDISTsim}
set.seed(12)
cointoss <- rbinom(50000, 10, 1/2)
phat <- cointoss/10
xtabs(~phat)
mean(phat)
sd(phat)
plot(xtabs(~phat),ylab= "count",xlab = expression(hat(p)))
```

The previous plot is an approximation to the *sampling distribution of $\hat{p}$*.  The mean of the sampling distribution of $\hat{p}$ is $\mu_{\hat{p}} = p$, and the standard deviation of the sampling distribution of $\hat{p}$, $\sigma_{\hat{p}} = \sqrt{p\times(1-p)/n}$.  The standard deviation of the simulated $\hat{p}$ values, the standard deviation of a statistic, is called a *standard error*.  In this simulation, the estimated mean of the statistic is written as $\hat{\mu}_{\hat{p}}=$ `r mean(phat)`. The standard error of $\hat{p}$, sometimes written as $SE(\hat{p})$ or $\sigma_{\hat{p}}$ is `r sd(phat)`.



## The Central Limit Theorem

Let $X_1, X_2,...,X_n$ be independent identically distributed random variables with mean $\mu$ and variance $\sigma^2$, both finite.  Then for any constant z,

$$\lim_{n \to \infty} P\left(\frac{\bar{X} - \mu}{\sigma/\sqrt{n}} \leq z \right) = \Phi(z) $$

Where $\Phi$ is the cdf of the standard normal distribution.  The central limit theorem means that for ``sufficiently large'' $n$, the sampling distribution of $\bar{X}$ is approximately normal with mean $\mu_{\bar{X}}= \mu$, and standard deviation $\sigma_{\bar{X}} = \sigma/\sqrt{n}$.  Recall that the standard deviation of a statistic is called a standard error.  Consequently, our text refers to $\sigma_{\bar{X}}$ as $SE(\bar{X})$.

#### Note:  If you one is sampling from a normal distribution, then the resulting sampling distribution of the sample mean is exactly normal.  It is always the case that $\mu_{\bar{X}} = \mu$ and $\sigma_{\bar{X}} = \sigma/\sqrt{n}$ regardless of the population one is sampling from.

## Just how large is ''sufficiently large''?

Consider the following simulations

```{r CLTsim, fig.height = 12, fig.width= 9}
set.seed(123)
par(mfrow=c(4,3))
# X~N(50,15)
x <- seq(0,100,.01)
y <- dnorm(x,50,15)
plot(x,y,type="l",col="tomato2",lwd=2,main="X~N(50,15)",xlab="",ylab="")

# X~U(0,1)
x <- seq(0,1,.001)
y <- dunif(x,0,1)
plot(x,y,type="l",col="tomato2",lwd=2,main="X~U(0,1)",xlab="",ylab="")

# X~Exp(1)
x <- seq(0,5,.01)
y <- dexp(x,1)
plot(x,y,type="l",col="tomato2",lwd=2,main="X~Exp(1)",xlab="",ylab="")

m <- 20000  # Number of Samples
EX <- 1.2   # Character expansion

xbar.5 <- apply(matrix(rnorm(m*5,50,15),nrow=m),1,mean)
hist(xbar.5,breaks="Scott",col="tomato2",xlim=c(0,100),prob=T,xlab="",ylab="",main="")
mtext(  expression(bar(x)[5]),side=3,line=1,cex=EX)

xbar.5 <- apply(matrix(runif(m*5,0,1),nrow=m),1,mean)
hist(xbar.5,breaks="Scott",col="tomato2",xlim=c(0,1),prob=T,xlab="",ylab="",main="")
mtext(  expression(bar(x)[5]),side=3,line=1,cex=EX)

xbar.5 <- apply(matrix(rexp(m*5,1),nrow=m),1,mean)
hist(xbar.5,breaks="Scott",col="tomato2",xlim=c(0,5),prob=T,xlab="",ylab="",main="")
mtext(  expression(bar(x)[5]),side=3,line=1,cex=EX)

xbar.10 <- apply(matrix(rnorm(m*10,50,15),nrow=m),1,mean)
hist(xbar.10,breaks="Scott",col="tomato2",xlim=c(0,100),prob=T,xlab="",ylab="",main="")
mtext(  expression(bar(x)[10]),side=3,line=1,cex=EX)

xbar.10 <- apply(matrix(runif(m*10,0,1),nrow=m),1,mean)
hist(xbar.10,breaks="Scott",col="tomato2",xlim=c(0,1),prob=T,xlab="",ylab="",main="")
mtext(  expression(bar(x)[10]),side=3,line=1,cex=EX)

xbar.10 <- apply(matrix(rexp(m*10,1),nrow=m),1,mean)
hist(xbar.10,breaks="Scott",col="tomato2",xlim=c(0,5),prob=T,xlab="",ylab="",main="")
mtext(  expression(bar(x)[10]),side=3,line=1,cex=EX)

xbar.30 <- apply(matrix(rnorm(m*30,50,15),nrow=m),1,mean)
hist(xbar.30,breaks="Scott",col="tomato2",xlim=c(0,100),prob=T,xlab="",ylab="",main="")
mtext(  expression(bar(x)[30]),side=3,line=1,cex=EX)

xbar.30 <- apply(matrix(runif(m*30,0,1),nrow=m),1,mean)
hist(xbar.30,breaks="Scott",col="tomato2",xlim=c(0,1),prob=T,xlab="",ylab="",main="")
mtext(  expression(bar(x)[30]),side=3,line=1,cex=EX)

xbar.30 <- apply(matrix(rexp(m*30,1),nrow=m),1,mean)
hist(xbar.30,breaks="Scott",col="tomato2",xlim=c(0,5),prob=T,xlab="",ylab="",main="")
mtext(  expression(bar(x)[30]),side=3,line=1,cex=EX)

par(mfrow=c(1,1))
```

#### Note:  The usual rule of thumb, found in most textbooks, is that the CLT is reasonably accurate if $n \geq 30$.  Such rules are wishful thinking, dating to a pre-computer age when one had few realistic alternatives to using the CLT because most other methods were computationally infeasible.


## CLT for Binomial Data

**Example 4.9** Toss a fair coin 300 times.  Find the approximate probability of getting at most 160 heads.

**Solution:** Let $X=$ the number of heads in 300 flips of a fair coin.  Since $X \sim Bin(n = 300, p = 1/2)$ it follows that $\mu_X = np$ and $\sigma_{X} =\sqrt{np\times(1 - p)}$, and $Y \dot{\sim} N(np, \sqrt{np\times(1 - p)})$.  It follows that $P(X \leq 160) = P\left(\frac{X - np}{\sqrt{np \times (1 - p)}} = Z \leq \frac{160 - 150}{\sqrt{300/4}}\right).$  Furthermore, $P(Z \leq 1.154701)=$ `r pnorm((160-150)/sqrt(300/4))`.  Solve the problem in class using $\hat{p}$ as the random variable.

**Exact Solution:** $P(X \leq 160)= \sum_{x = 0}^{x = 160}\binom{300}{x} 0.5^x 0.5^{300-x}=$ `r pbinom(160, 300, 1/2)`.

```{r ES}
pbinom(160, 300, 1/2)
```

## Student $t$ distribution

```{r}
set.seed(123)
sims <- 10000 - 1
Zsim <- numeric(sims)
Tsim <- numeric(sims)
n <- 4
MU <- 100
SIGMA <- 2
for(i in 1:sims){
  RS <- rnorm(n, MU, SIGMA)
  Zsim[i] <- (mean(RS) - MU)/(SIGMA/sqrt(n))
  Tsim[i] <- (mean(RS) - MU)/(sd(RS)/sqrt(n))
}
```


```{r}
plot(density(Zsim), col = "red", main = "")
curve(dnorm, -4, 4, add = TRUE, col = "blue")
```

Add your comments here

```{r}
plot(density(Tsim), col = "red", main = "", xlim = c(-6, 6))
curve(dt(x, df = n - 1), -6, 6, add = TRUE, col = "blue")
```

Add your comments here

```{r}
curve(dnorm, -6, 6, col = "blue", ylab = "")
curve(dt(x, df = n - 1), -6, 6, add = TRUE, col = "red")
```

```{r}
library(PASWR2)
ggplot(data = WATER, aes(x = source, y = sodium, fill = source)) + 
  geom_boxplot() +
  guides(fill = FALSE) + 
  scale_fill_brewer() + 
  theme_bw()
t.test(sodium ~ source, data = WATER)
```


## Permutation Approach

```{r}
sims <- 10000 - 1
ts <- numeric(sims)
tobs <- t.test(sodium ~ source, data = WATER)$stat
for(i in 1:sims){
  ts[i] <- t.test(sodium ~ sample(source), data = WATER)$stat
}
pv <- (sum(ts <= tobs)*2 + 1)/(sims + 1)
pv
ggplot(data = data.frame(x = ts), aes(x = x)) + 
  geom_density(fill = "pink") +
  theme_bw() +
  stat_function(fun = dt, args=list(df = 22), color = "red")
```

Can A New Drug Reduce the Spread of Schistosomiasis?
========================================================

Schistosomiasis (skis-tuh-soh-may'-uh-sis) is a disease in humans caused by parasitic flatworms called schistosomes (skis'-tuh-sohms).  Schistosomiasis affects about 200 million people worldwide and is a serious problem in sub-Saharan Africa, South America, China, and Southeast Asia.  The disease can cause death, but more commonly results in chronic and debilitating symptoms, arising primarily from the body's immune reaction to parasite eggs lodged in the liver, spleen,and intestines.

Currently there is one drug, praziquantel (pray'-zee-kwan-tel), in common use for treatment of schistosomiasis; it is inexpensive and effective. However, many organizations are concerned about relying on a single drug to treat a serious disease that affects so many people worldwide. Drug resistance may have prompted an outbreak in the 1990s in Senegal, where cure rates were low.  In 2007, several researchers published work on a  promising drug called K11777, which, in theory, might treat schistosomiasis.

```{r F1}
gender <- c(rep("Female", 10), rep("Male", 10)) 
group <- rep(rep(c("Treatment", "Control"), each = 5), 2)
worms <- c(1, 2, 2, 10, 7, 16, 10, 10, 7, 17, 3, 5, 9, 10, 6, 31, 26, 28, 13, 47)
schis <- data.frame(gender, group, worms)
head(schis, n = 3)
schis
```

```{r F2, fig.width=7, fig.height= 3.5}
require(ggplot2)
p <- ggplot(data = schis, aes(group, worms)) + 
  geom_point(position = "jitter", aes(color = group)) + 
  facet_grid(. ~ gender) + 
  theme_bw()
p
```

ACTIVITY
==================================================

1. Use the previous graph to compare visually the number of worms for the treatment and control groups for both the male and female mice.  Does each of the four groups appear to have a similar center and similar spread?  Are there any outliers (extreme observations that don't fit with the rest of the data)?

2. Calculate appropriate summary statistics (e.g., the median, mean, and standard deviation) for each of the four groups.  For the female mice, calculate the difference between the treatment and control means.  Do the same for the male mice.

```{r ORDER}
with(data = schis, 
schis[order(gender, group, worms),]
     )
```

```{r STATS}
with(data = schis, 
tapply(worms, list(gender, group), median)    
     )
with(data = schis, 
tapply(worms, list(gender, group), mean)   
     )
with(data = schis, 
tapply(worms, list(gender, group), sd)    
     )
```

The descriptive analysis in Questions 1 and 2 points to a positive treatment effect: K11777 appears to have reduced the number of parasite worms in this sample.  But descriptive statistics are usually only the first step in ascertaining whether an effect is real; we often conduct a significance test or create a confidence interval to determine if chance alone could explain the effect.

We will introduce the basic concepts of randomization tests in a setting where units (mice in this example) are randomly allocated to a treatment or control group.  Using a significance test, we will decide if an observed treatment effect (the difference between the mean responses in the treatment and control) is "real" or if "random chance alone" could plausibly explain the observed effect.  The null hypothesis states that "random chance alone" is the reason for the observed effect.  In this initial discussion, the alternative hypothesis will be one-sided because we want to show that the true treatment mean($\mu_{treatment}$) is less than the true control mean ($\mu_{control}$).

Statistical Inference Through a Randomization Test
==================================================

Whether they take the form of significance tests or confidence intervals, inferential procedures rest on the **fundamental question for inference:** "What would happen if we did this many times?" Let's unpack this question in the context of the female mice schistosomiasis.  We observed a difference in means of 7.6 = 12 - 4.4 worms between the control and treatment groups.  While we expect that this large difference reflects the effectiveness of the drug, it is possible that chance alone could explain this difference.  This "chance alone" position is usually called the null hypothesis and includes the following assumptions:
* The number of parasitic worms found in the liver naturally varies from mouse to mouse.
* Whether or not the drug is effective, there clearly is variability in the responses of mice to the infestation of schistosomes.
* Each group exhibits this variability, and even if the drug is not effective, some mice do better than others.
* The only explanation for the observed difference of 7.6 worms in the means is that the random allocation randomly placed mice with  larger numbers of worms in the control group and mice with smaller numbers of worms in the treatment group.

In this study, the **null hypothesis** is that the treatment has no effect on the average worm count, and it is denoted as 

> $H_0:\mu_{control} = \mu_{treatment}$
Another way to write this hypothesis is 
> $H_0: \text{the treatment has no effect on average worm count}$

Alternative hypotheses can be "one-sided, greater-than" (as in this investigation), "one-sided, less-than" (the treatment causes an increase in worm count), or "two-sided" (the treatment is different, in one direction or the other, from the mean).  We chose to test a one-sided hypothesis because there is a clear research interest in one direction.  In other words, we will take action (start using the drug) only if we can show that K11777 reduces worm count.

> **The fundamental question for inference:** Every statistical inference procedure is based on the question "How does what we observed in our data compare to what would happen if the null hypothesis were actually true and we repeated the process many times?"

For a randomization test comparing responses for the two groups, this question becomes "How does the observed difference between groups compare to what would happen if the treatments actually had no effect on the individual responses and we repeated the random allocation of individuals to groups many times?"

Conducting a Randomization Test by Hand
===================================================

1. To get the feel for the concept of a _p_-value, write each of the female worm counts on an index card.  Shuffle the 10 index cards, and then draw five cards at random (without replacement).  Call these five cards the treatment group and the five remaining cards the control group.  Under the null hypothesis (i.e. the treatment has no effect on the worms), this allocation mimics precisely what actually happened in our experiment, since the only cause of group differences is the random allocation.  Calculate the mean of the five cards representing the treatment group and the mean of the five cards representing the control group.  Then find the difference between the control and treatment group means that you obtained in your allocation.  To be consistent, take the control group mean minus the treatment group mean.

2. If you were to do another random allocation, would you get the same difference in means? Explain.

3. Now, perform nine more random allocations, each time computing and writing down the difference in mean worm count between the control group and the treatment group.  Graphically represent the differences.  What proportion of these differences are 7.6 or larger?

4.  If you performed the simulation many times, would you expect a large percentage of the simulations to result in a mean difference greater than 7.6?  Explain.

The reasoning in the previous activity leads us to the randomization test and an interpretation of the fundamental question for inference.  The fundamental question for this context is as follows:  "If the null hypothesis were actually true and we randomly allocated our 10 mice to treatment and control groups many times, what proportion of the time would the observed difference in means be as big or bigger than 7.6?"  This long-run proportion is a probability that statisticians call the **_p-value_** of the randomization test.  The _p_-values for most randomization tests are found through simulations.  Despite the fact that simulations do not give exact _p_-values, they are usually preferred over the tedious and time consuming process of listing all possible outcomes.  Researchers usually pick a round number such as 10,000 repetitions on the simulation and approximate the _p_-value accordingly.  Since this _p_-value is an approximation, it is often referred to as the **empirical _p_-value.**

> **Key Concept:** Assuming that nothing except the random allocation process is creating group differences, the _p_-value of a randomization test is the probability of obtaining a group difference as large as or larger than the group difference actually observed in the experiment.

> **Key Concept:** The calculation of an empirical _p_-value requires these steps:

* Repeat the random allocation process a number of times (N times).
* Record, each time, whether or not the group difference exceeds or is the same as the one observed in the actual experiment (let X be the number of times the group difference exceeds or is the same as the one observed).
* Compute (X + 1)/(N + 1) to get the _p_-value, the proportion of times the difference exceeds or is the same as the observed difference.

**Note:** Including the observed value as one of the possible allocations is a more conservative approach and protects against getting a _p_-value of 0.  Our observation from the actual experiment provides evidence that the true _p_-value is greater than zero.

Performing a Randomization Test Using Computer Simulation
=========================================================

While physical simulations (such as the index cards activity) help us understand the process of computing an empirical _p_-value, using computer software is a much more efficient way of producing an empirical _p_-value based on a large number of iterations.  If you are simulating 10 random allocations, it is just as easy to use index cards as a computer.  However, the advantage of a computer simulation is that 10,000 random allocations can be conducted in almost the same time it takes to simulate 10 allocations.

Two-Sample Permutation Test
=============================
> Pool the _m + n_ values.
* **repeat**
  * Draw a resample of size _m_ without replacement.
  * Use the remaining _n_ observations for the other sample.
  * Calculate the difference in means or another test statistic that compares samples.
* **until** 
  * we have enough samples

Calculate the _p_-value as the fraction of times the random statistics are more or as extreme as the original statistic. Optionally, plot a histogram of the random statistic values.   
 
**Definition 3.2** A *test statistic* is a numerical function of the data whose value determines the result of the test.  The function itself is generally denoted *T=T(X)*, where **X** represents the data.  After being evaluated for the sample data **x**, the result is called an *observed test statistic* and is written in lowercase, *t=T(x)*.


Using Computer Simulations to Conduct a Hypothesis Test
=========================================================

1. Write code to allocate randomly each of the female worm counts to either the treatment or the control group.

2. Take the control group average minus the K11777 treatment group average.

3. Write code that perform steps 1. and 2. N = 99,999 times.  Compute and report the empirical _p_-value.

4. Create a histogram of the N simulated differences between group means and comment of the shape of the histogram.  

5. Based on your results in Questions 3 and 4 and assuming the null hypothesis is true, about how frequently do you think you would obtain a mean difference as large as or larger than 7.6 by random allocation alone?

6. Does your answer to Question 5 lead you to believe the "chance alone" position (i.e., the null hypothesis that the mean worm count is the same for both the treatment and the control), or does is lead you to believe that K11777 has a positive inhibitory effect on the schistosome worm in female mice?  Explain.

Using R
==========

```{r APPLY}
ND <- schis[gender=="Female", ]
ND
tapply(ND$worms, ND$group, mean)
# OR
ANS1 <- with(data = ND,
             tapply(worms, group, mean)
             )
ANS1
observed <- ANS1[1] - ANS1[2]
observed
names(observed) <- NULL
observed
```
Since we will be working with the **worms** variable for females only, we will create a vector holding these values.  Then, we will draw a random sample of size 5 from the numbers 1 through 10 (there are 10 observations).  The worms values corresponding to these positions will be values for the Control group and the remaining ones for the Treatment group.  The mean difference of this permutation will be stored in **result**.  This will be repeated many times.

```{r F3}
Worms <- ND$worms
Worms
# Another way:
Worms2 <- subset(ND, select = worms, drop = TRUE)
Worms2
N <- 10^4 - 1         # number of times to repeat the process
result <- numeric(N)  # space to save the random differences
set.seed(5)
for (i in 1:N){
  # sample of size 5, from 1 to 10, without replacement
  index <- sample(10, size = 5, replace = FALSE)
  result[i] <- mean(Worms2[index]) - mean(Worms2[-index])
}
hist(result, col = "blue", freq = FALSE, main = "", breaks = "Scott")
d.res <- density(result)
plot(d.res, main ="", xlab = "", ylab = "")
polygon(d.res, col ="pink")
xs <- c(7.6, d.res$x[d.res$x >= 7.6])
ys <- c(0, d.res$y[d.res$x>=7.6])
polygon(xs, ys, col = "red")
pvalue <- (sum(result >= observed) + 1)/(N + 1) # p-value
pvalue  # results will vary
# ggplot2 approach now
DF <- data.frame(x = result)
p <- ggplot(data = DF) + geom_density(aes(x = x, y = ..density..), fill = 'pink', alpha = 0.4) + theme_bw()
p
x.dens <- density(result)
df.dens <- data.frame(x = x.dens$x, y = x.dens$y)
p + geom_area(data = subset(df.dens, x >= 7.6 & x <= max(DF$x)), aes(x = x, y = y), fill = 'blue', alpha = 0.4) + labs(x = expression(bar(x)[Control] - bar(x)[Treatment]), y = '', title = "Randomization Distribution") + theme_bw()
```
The code snippet **result >= observed** results in a vector of **TRUE's** and **FALSE's** depending on whether or not the mean difference computed for a resample is greater than the observed mean difference.  **sum(result >= observed)** counts the number of **TRUE's**.  Thus, the computed _p_-value is just the proportion of statistics (including the original) that are as large or larger than the original mean difference. The empirical _p_-value is `r pvalue`.

Because the sample sizes in the schistosomiasis study are small, it is possible to apply mathematical methods to obtain an **exact _p_-value** for this randomization test.  An exact _p_-value can be obtained by writing down the set of all possibilities (assuming each possible outcome is equally likely under the null hypothesis) and then calculating the proportion of the set for which the difference is at least as large as the observed difference.  In the schistosomiasis study, this requires listing every possible combination in which five of the 10 female mice can be allocated to the treatment (and the other five mice are assigned to the control).  There are $\binom{10}{5}=$ `r choose(10,5)` possible combinations.  For each of these combinations, the difference between the treatment and control means is then calculated.  The exact _p_-value is the proportion of times in which the difference in the means is at least as large as the observed difference of 7.6 worms.  Of these 252 combinations, six have a mean difference of 7.6 and one has a mean difference greater than 7.6 (namely 8.8).  Since all 252 of these random allocations are equally likely, the exact _p_-value in this example is 7/252 = 0.0278.  However, most real studies are too large to list all possible samples.  Randomization tests are almost always adequate, providing approximate _p_-values that are close enough to the true _p_-value.  

> Key Concept: The larger the number of randomizations within a simulation study, the more precise the _p_-value is. If the true _p_-value is p, the estimated _p_-value has variance approximately equal to $p(1 - p)/N$, where $N$ is the number of resamples.

Sometimes we have some threshold _p_-value at or below which we will reject the null hypothesis and conclude in favor of the alternative hypothesis.  This threshold value is called a **significance level** and is usually denoted by the Greek letter alpha ($\alpha$).  Common values are $\alpha = 0.05$ and $\alpha = 0.01$, but the value will depend heavily on context and the researcher's assessment of the acceptable risk of stating an incorrect conclusion.  When the study's _p_-value is less than or equal to this significance level, we state that the results are **statistically significant at level $\alpha$**.  If you see the phrase "statistically significant" without a specification of $\alpha$ the writer is most likely assuming $\alpha = 0.05$, for reasons of history and convention alone.  However, it is best to show the _p_-value instead of simply stating a result is significant at a particular $\alpha$-level.

## Approach Two: Welch Test

```{r}
t.test(worms ~ group, data = ND)
set.seed(123)
sims <- 10000 - 1
ts <- numeric(sims)
tobs <- t.test(worms ~ group, data = ND)$stat
for(i in 1:sims){
  ts[i] <-  t.test(worms ~ sample(group), data = ND)$stat
}
pv <- (sum(ts >= tobs) + 1)/(sims + 1)
pv
ggplot(data = data.frame(x = ts), aes(x = x)) +
  geom_density(fill = "purple", alpha = 0.2) +
  theme_bw() + 
  labs(x = "t")
```

## Add more notes here